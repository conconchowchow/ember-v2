{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity Testbench\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ember Package Testing (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, os\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kathleenge/Desktop/NON/ember-v2\n"
     ]
    }
   ],
   "source": [
    "# fixing dependencies if current path is <root>/src/ember/examples/diversity_testbench.ipynb\n",
    "target_dir = 'src/ember/examples'\n",
    "if os.getcwd()[-18:] == target_dir:\n",
    "    os.chdir('../../..')\n",
    "print(os.getcwd())\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kathleenge/desktop/non/ember-v2\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: things below this are to install required dependencies (only do this the venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -e .\n",
    "# %pip install -q google-generativeai==0.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ember Repo Loads (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig\n",
    "from ember.core.registry.model.config.settings import initialize_ember\n",
    "from ember.core.registry.model.base.services.model_service import ModelService\n",
    "from ember.core.registry.model.base.schemas.model_info import ModelInfo\n",
    "from ember.core.registry.model.base.schemas.cost import ModelCost, RateLimit\n",
    "from ember.core.registry.model.base.schemas.provider_info import ProviderInfo\n",
    "\n",
    "from ember.core.registry.model import load_model, ChatResponse\n",
    "from ember.core.registry.model.base.services.model_service import ModelService\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "4 validation errors for EmberSettings\nregistry.models.3.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error\nregistry.models.4.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error\nregistry.models.5.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error\nregistry.models.6.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_registry \u001b[38;5;241m=\u001b[39m initialize_ember()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_registry\u001b[38;5;241m.\u001b[39mlist_models())\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m ModelService(registry\u001b[38;5;241m=\u001b[39mmodel_registry)\n",
      "File \u001b[0;32m~/Desktop/NON/ember-v2/src/ember/core/registry/model/config/settings.py:252\u001b[0m, in \u001b[0;36minitialize_ember\u001b[0;34m(config_path, auto_register, auto_discover)\u001b[0m\n\u001b[1;32m    249\u001b[0m settings_obj\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mauto_register \u001b[38;5;241m=\u001b[39m auto_register\n\u001b[1;32m    250\u001b[0m settings_obj\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mauto_discover \u001b[38;5;241m=\u001b[39m auto_discover\n\u001b[0;32m--> 252\u001b[0m registry_instance: ModelRegistry \u001b[38;5;241m=\u001b[39m _initialize_model_registry(settings\u001b[38;5;241m=\u001b[39msettings_obj)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m registry_instance\n",
      "File \u001b[0;32m~/Desktop/NON/ember-v2/src/ember/core/registry/model/config/settings.py:184\u001b[0m, in \u001b[0;36m_initialize_model_registry\u001b[0;34m(settings)\u001b[0m\n\u001b[1;32m    181\u001b[0m merged_config \u001b[38;5;241m=\u001b[39m resolve_env_vars(data\u001b[38;5;241m=\u001b[39mmerged_config)\n\u001b[1;32m    182\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal merged config keys: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(merged_config\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m--> 184\u001b[0m final_settings: EmberSettings \u001b[38;5;241m=\u001b[39m EmberSettings(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmerged_config)\n\u001b[1;32m    185\u001b[0m registry: ModelRegistry \u001b[38;5;241m=\u001b[39m ModelRegistry(logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    187\u001b[0m discovered_models: Dict[\u001b[38;5;28mstr\u001b[39m, ModelInfo] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic_settings/main.py:176\u001b[0m, in \u001b[0;36mBaseSettings.__init__\u001b[0;34m(__pydantic_self__, _case_sensitive, _nested_model_default_partial_update, _env_prefix, _env_file, _env_file_encoding, _env_ignore_empty, _env_nested_delimiter, _env_nested_max_split, _env_parse_none_str, _env_parse_enums, _cli_prog_name, _cli_parse_args, _cli_settings_source, _cli_parse_none_str, _cli_hide_none_type, _cli_avoid_json, _cli_enforce_required, _cli_use_class_docs_for_groups, _cli_exit_on_error, _cli_prefix, _cli_flag_prefix_char, _cli_implicit_flags, _cli_ignore_unknown_args, _cli_kebab_case, _secrets_dir, **values)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    148\u001b[0m     __pydantic_self__,\n\u001b[1;32m    149\u001b[0m     _case_sensitive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalues: Any,\n\u001b[1;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__pydantic_self__\u001b[38;5;241m.\u001b[39m_settings_build_values(\n\u001b[1;32m    178\u001b[0m             values,\n\u001b[1;32m    179\u001b[0m             _case_sensitive\u001b[38;5;241m=\u001b[39m_case_sensitive,\n\u001b[1;32m    180\u001b[0m             _nested_model_default_partial_update\u001b[38;5;241m=\u001b[39m_nested_model_default_partial_update,\n\u001b[1;32m    181\u001b[0m             _env_prefix\u001b[38;5;241m=\u001b[39m_env_prefix,\n\u001b[1;32m    182\u001b[0m             _env_file\u001b[38;5;241m=\u001b[39m_env_file,\n\u001b[1;32m    183\u001b[0m             _env_file_encoding\u001b[38;5;241m=\u001b[39m_env_file_encoding,\n\u001b[1;32m    184\u001b[0m             _env_ignore_empty\u001b[38;5;241m=\u001b[39m_env_ignore_empty,\n\u001b[1;32m    185\u001b[0m             _env_nested_delimiter\u001b[38;5;241m=\u001b[39m_env_nested_delimiter,\n\u001b[1;32m    186\u001b[0m             _env_nested_max_split\u001b[38;5;241m=\u001b[39m_env_nested_max_split,\n\u001b[1;32m    187\u001b[0m             _env_parse_none_str\u001b[38;5;241m=\u001b[39m_env_parse_none_str,\n\u001b[1;32m    188\u001b[0m             _env_parse_enums\u001b[38;5;241m=\u001b[39m_env_parse_enums,\n\u001b[1;32m    189\u001b[0m             _cli_prog_name\u001b[38;5;241m=\u001b[39m_cli_prog_name,\n\u001b[1;32m    190\u001b[0m             _cli_parse_args\u001b[38;5;241m=\u001b[39m_cli_parse_args,\n\u001b[1;32m    191\u001b[0m             _cli_settings_source\u001b[38;5;241m=\u001b[39m_cli_settings_source,\n\u001b[1;32m    192\u001b[0m             _cli_parse_none_str\u001b[38;5;241m=\u001b[39m_cli_parse_none_str,\n\u001b[1;32m    193\u001b[0m             _cli_hide_none_type\u001b[38;5;241m=\u001b[39m_cli_hide_none_type,\n\u001b[1;32m    194\u001b[0m             _cli_avoid_json\u001b[38;5;241m=\u001b[39m_cli_avoid_json,\n\u001b[1;32m    195\u001b[0m             _cli_enforce_required\u001b[38;5;241m=\u001b[39m_cli_enforce_required,\n\u001b[1;32m    196\u001b[0m             _cli_use_class_docs_for_groups\u001b[38;5;241m=\u001b[39m_cli_use_class_docs_for_groups,\n\u001b[1;32m    197\u001b[0m             _cli_exit_on_error\u001b[38;5;241m=\u001b[39m_cli_exit_on_error,\n\u001b[1;32m    198\u001b[0m             _cli_prefix\u001b[38;5;241m=\u001b[39m_cli_prefix,\n\u001b[1;32m    199\u001b[0m             _cli_flag_prefix_char\u001b[38;5;241m=\u001b[39m_cli_flag_prefix_char,\n\u001b[1;32m    200\u001b[0m             _cli_implicit_flags\u001b[38;5;241m=\u001b[39m_cli_implicit_flags,\n\u001b[1;32m    201\u001b[0m             _cli_ignore_unknown_args\u001b[38;5;241m=\u001b[39m_cli_ignore_unknown_args,\n\u001b[1;32m    202\u001b[0m             _cli_kebab_case\u001b[38;5;241m=\u001b[39m_cli_kebab_case,\n\u001b[1;32m    203\u001b[0m             _secrets_dir\u001b[38;5;241m=\u001b[39m_secrets_dir,\n\u001b[1;32m    204\u001b[0m         )\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 4 validation errors for EmberSettings\nregistry.models.3.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error\nregistry.models.4.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error\nregistry.models.5.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error\nregistry.models.6.api_key\n  Value error, No API key provided or defaulted. [type=value_error, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "model_registry = initialize_ember()\n",
    "print(model_registry.list_models())\n",
    "llm = ModelService(registry=model_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai:gpt-4o',\n",
       " 'openai:gpt-4o-mini',\n",
       " 'openai:o1',\n",
       " 'anthropic:claude-3.5-sonnet',\n",
       " 'google:gemini-1.5-pro',\n",
       " 'google:gemini-2.0-flash',\n",
       " 'google:gemini-exp-1206']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids: List[str] = [\n",
    "            \"openai:o1\",\n",
    "            \"openai:gpt-4o\",\n",
    "            \"openai:gpt-4o-mini\",\n",
    "            # \"anthropic:claude-3.5-sonnet\", # API key not working\n",
    "            # \"invalid:model\",  # Expected to trigger an error.\n",
    "            # \"google:model/gemini-1.5-pro\", # need to fix model alignment\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸ Testing model: openai:o1\n",
      "ðŸ›Žï¸ Service response from openai:o1:\n",
      "Quantum computing uses quantum bits, or qubits, which can exist in superpositions of states and become entangled. These properties enable massive parallel processing, making certain computations exponentially faster than classical computers. By harnessing quantum phenomena, quantum computing aims to solve complex problems, including cryptography, optimization, and simulations of quantum systems.\n",
      "\n",
      "ðŸŽ¯ Direct response from openai:o1:\n",
      "The capital of France is Paris.\n",
      "\n",
      "âž¡ï¸ Testing model: openai:gpt-4o\n",
      "ðŸ›Žï¸ Service response from openai:gpt-4o:\n",
      "Quantum computing harnesses quantum mechanics to process information using qubits, which can exist in multiple states simultaneously. This allows for potentially exponential increases in computational power compared to classical computers, enabling the solving of complex problems much more efficiently, such as cryptography, optimization, and simulations of quantum systems.\n",
      "\n",
      "ðŸŽ¯ Direct response from openai:gpt-4o:\n",
      "The capital of France is Paris.\n",
      "\n",
      "âž¡ï¸ Testing model: openai:gpt-4o-mini\n",
      "ðŸ›Žï¸ Service response from openai:gpt-4o-mini:\n",
      "Quantum computing harnesses the principles of quantum mechanics to process information. Unlike classical bits, which represent either 0 or 1, quantum bits (qubits) can exist in multiple states simultaneously. This enables faster problem-solving for complex tasks, such as cryptography and optimization, by exploiting phenomena like superposition and entanglement.\n",
      "\n",
      "ðŸŽ¯ Direct response from openai:gpt-4o-mini:\n",
      "The capital of France is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_id in model_ids:\n",
    "            try:\n",
    "                print(f\"âž¡ï¸ Testing model: {model_id}\")\n",
    "\n",
    "                # Two usage styles are demonstrated below:\n",
    "                # 1. Service-based invocation: Recommended for automatic usage tracking.\n",
    "                service_response: ChatResponse = llm.invoke_model(\n",
    "                    model_id=model_id,\n",
    "                    prompt=\"Explain quantum computing in 50 words\",\n",
    "                )\n",
    "                print(f\"ðŸ›Žï¸ Service response from {model_id}:\\n{service_response.data}\\n\")\n",
    "\n",
    "                # 2. Direct model instance usage: Useful for more granular or PyTorch-like workflows.\n",
    "                model = load_model(model_id=model_id, registry=model_registry)\n",
    "                direct_response: ChatResponse = model(\n",
    "                    prompt=\"What's the capital of France?\"\n",
    "                )\n",
    "                print(f\"ðŸŽ¯ Direct response from {model_id}:\\n{direct_response.data}\\n\")\n",
    "\n",
    "            except Exception as error:\n",
    "                print(f\"âŒ Error with model {model_id}: {str(error)}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register an OpenAI GPT-4o model\n",
    "# openai_info = ModelInfo(\n",
    "#     model_id=\"openai:gpt-4o\",\n",
    "#     model_name=\"gpt-4o\",\n",
    "#     cost=ModelCost(input_cost_per_thousand=0.03, output_cost_per_thousand=0.06),\n",
    "#     rate_limit=RateLimit(tokens_per_minute=80000, requests_per_minute=5000),\n",
    "#     provider=ProviderInfo(name=\"OpenAI\", default_api_key=openai_key),\n",
    "#     api_key=openai_key,\n",
    "# )\n",
    "# model_registry.register_model(openai_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = llm(prompt=\"Hello!\", model_id=\"openai:o1\")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Neural Similarity Scoring - Cosine Similarity (WIP)\n",
    "\n",
    "- from `src/ember/core/utils/embedding_utils.py`\n",
    "- from jason\n",
    "- need to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Protocol\n",
    "import math\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 1) Embedding Model Interfaces & Implementations\n",
    "################################################################\n",
    "\n",
    "\n",
    "class EmbeddingModel(Protocol):\n",
    "    \"\"\"Interface for embedding models.\n",
    "\n",
    "    This protocol defines the minimal interface required to compute a text\n",
    "    embedding. Implementations may use local models, external APIs, or custom\n",
    "    neural networks.\n",
    "\n",
    "    Methods:\n",
    "        embed_text: Compute the embedding for a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Computes the embedding vector for the provided text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list of floats representing the embedding vector.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "class Text_Embedding_3_EmbeddingModel(Protocol):\n",
    "    \"\"\"Interface for embedding models.\n",
    "\n",
    "    This protocol defines the minimal interface required to compute a text\n",
    "    embedding. Implementations may use local models, external APIs, or custom\n",
    "    neural networks.\n",
    "\n",
    "    Methods:\n",
    "        embed_text: Compute the embedding for a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initializes the embedding model with the OpenAI API key.\n",
    "\n",
    "        Args:\n",
    "            api_key (str): OpenAI API key for authentication.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenAI API key must be provided or set in the environment variable OPENAI_API_KEY.\")\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Computes the embedding vector for the provided text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list of floats representing the embedding vector.\n",
    "        \"\"\"\n",
    "        response = openai.Embedding.create(\n",
    "            model=\"text-embedding-3\",\n",
    "            input=text\n",
    "        )\n",
    "        return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "class MockEmbeddingModel:\n",
    "    \"\"\"Mock implementation of an embedding model using naive ASCII encoding.\n",
    "\n",
    "    This simple model converts each character in the text to a normalized ASCII\n",
    "    value. It is intended solely for demonstration and testing purposes.\n",
    "\n",
    "    Methods:\n",
    "        embed_text: Converts text to a sequence of normalized ASCII values.\n",
    "    \"\"\"\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Embeds text by converting each character to its normalized ASCII code.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list of floats representing the embedding. Returns an\n",
    "            empty list if the text is empty.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        return [ord(ch) / 256.0 for ch in text]\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 2) Similarity Metric Interface & Implementations\n",
    "################################################################\n",
    "\n",
    "\n",
    "class SimilarityMetric(ABC):\n",
    "    \"\"\"Abstract base class for computing similarity between embedding vectors.\n",
    "\n",
    "    Subclasses must implement the similarity method to calculate a similarity\n",
    "    score between two vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
    "        \"\"\"Calculates the similarity between two embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            vec_a (List[float]): The first embedding vector.\n",
    "            vec_b (List[float]): The second embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The similarity score, typically in the range [0, 1] or [-1, 1].\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class CosineSimilarity(SimilarityMetric):\n",
    "    \"\"\"Implementation of cosine similarity for embedding vectors.\n",
    "\n",
    "    The cosine similarity is defined as:\n",
    "        similarity(a, b) = (a Â· b) / (||a|| * ||b||)\n",
    "\n",
    "    Returns 0.0 if either vector is empty or if any vector's norm is zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
    "        \"\"\"Computes cosine similarity between two embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            vec_a (List[float]): The first embedding vector.\n",
    "            vec_b (List[float]): The second embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The cosine similarity score.\n",
    "        \"\"\"\n",
    "        if not vec_a or not vec_b:\n",
    "            return 0.0\n",
    "\n",
    "        dot_product: float = sum(a * b for a, b in zip(vec_a, vec_b))\n",
    "        norm_a: float = math.sqrt(sum(a * a for a in vec_a))\n",
    "        norm_b: float = math.sqrt(sum(b * b for b in vec_b))\n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 3) High-Level Utility Function\n",
    "################################################################\n",
    "\n",
    "\n",
    "def calculate_text_similarity(\n",
    "    text1: str, text2: str, model: EmbeddingModel, metric: SimilarityMetric\n",
    ") -> float:\n",
    "    \"\"\"Calculates text similarity using an embedding model and a similarity metric.\n",
    "\n",
    "    This function generates embeddings for the provided texts and then computes a\n",
    "    similarity score using the given similarity metric.\n",
    "\n",
    "    Args:\n",
    "        text1 (str): The first text string.\n",
    "        text2 (str): The second text string.\n",
    "        model (EmbeddingModel): An instance conforming to the embedding model interface.\n",
    "        metric (SimilarityMetric): An instance implementing a similarity metric.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed similarity score.\n",
    "    \"\"\"\n",
    "    embedding1: List[float] = model.embed_text(text=text1)\n",
    "    embedding2: List[float] = model.embed_text(text=text2)\n",
    "    return metric.similarity(vec_a=embedding1, vec_b=embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mock_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m text_a: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m text_b: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world??\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m score: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m calculate_text_similarity(\n\u001b[0;32m----> 8\u001b[0m     text1\u001b[38;5;241m=\u001b[39mtext_a, text2\u001b[38;5;241m=\u001b[39mtext_b, model\u001b[38;5;241m=\u001b[39mmock_model, metric\u001b[38;5;241m=\u001b[39mcosine\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity between \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_a\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mock_model' is not defined"
     ]
    }
   ],
   "source": [
    "mock_model: MockEmbeddingModel = MockEmbeddingModel()\n",
    "cosine: CosineSimilarity = CosineSimilarity()\n",
    "\n",
    "text_a: str = \"Hello world!\"\n",
    "text_b: str = \"Hello, world??\"\n",
    "\n",
    "score: float = calculate_text_similarity(\n",
    "    text1=text_a, text2=text_b, model=mock_model, metric=cosine\n",
    ")\n",
    "print(f\"Similarity between '{text_a}' and '{text_b}': {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Compression Ratio (WIP)\n",
    "\n",
    "from `src/ember/core/utils/eval/evaluators.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q diversity==0.2.0\n",
    "%pip install -q spacy==3.8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "from typing import Any, Dict, TypeVar, Optional, List, Generic, Callable, Union\n",
    "\n",
    "from ember.core.utils.eval.base_evaluator import IEvaluator, EvaluationResult\n",
    "from ember.core.utils.eval.extractors import RegexExtractor\n",
    "\n",
    "from diversity import compression_ratio\n",
    "\n",
    "T_out = TypeVar(\"T_out\")\n",
    "T_truth = TypeVar(\"T_truth\")\n",
    "\n",
    "\n",
    "class ComposedEvaluator(IEvaluator[T_out, T_truth], Generic[T_out, T_truth]):\n",
    "    \"\"\"Combines an output extractor with an evaluator for the extracted data.\n",
    "\n",
    "    This evaluator first transforms the system output using the provided extractor,\n",
    "    then evaluates the extracted value using the specified base evaluator.\n",
    "\n",
    "    Args:\n",
    "        extractor: An object with an `extract` method to process the system output.\n",
    "        base_evaluator (IEvaluator): An evaluator that processes the extracted output.\n",
    "\n",
    "    Returns:\n",
    "        EvaluationResult: The result of the evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        extractor: Any,  # Expecting an extractor with an `extract` method.\n",
    "        base_evaluator: IEvaluator[Any, Any],\n",
    "    ) -> None:\n",
    "        self.extractor = extractor\n",
    "        self.base_evaluator = base_evaluator\n",
    "\n",
    "    def evaluate(\n",
    "        self, system_output: T_out, correct_answer: Any, **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"Evaluates the provided system output against the correct answer.\n",
    "\n",
    "        Args:\n",
    "            system_output (T_out): The raw output generated by the system.\n",
    "            correct_answer (Any): The expected correct answer.\n",
    "            **kwargs: Additional keyword arguments for extraction or evaluation.\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: The result of evaluating the extracted value.\n",
    "        \"\"\"\n",
    "        extracted_value = self.extractor.extract(system_output, **kwargs)\n",
    "        return self.base_evaluator.evaluate(extracted_value, correct_answer, **kwargs)\n",
    "\n",
    "\n",
    "# Basic Evaluators\n",
    "\n",
    "\n",
    "class ExactMatchEvaluator(IEvaluator[str, str]):\n",
    "    \"\"\"Evaluator to check for an exact match between two strings,\n",
    "    ignoring differences in whitespace and case.\n",
    "\n",
    "    Example:\n",
    "        evaluator = ExactMatchEvaluator()\n",
    "        result = evaluator.evaluate(\"Hello World\", \"hello   world\")\n",
    "\n",
    "    Args:\n",
    "        compare_fn (Optional[Callable[[str, str], bool]]): Optional custom comparison function.\n",
    "            If not provided, strings are normalized (whitespace removed, lowercase) before comparison.\n",
    "\n",
    "    Returns:\n",
    "        EvaluationResult: The result containing a correctness flag and a score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, compare_fn: Optional[Callable[[str, str], bool]] = None) -> None:\n",
    "        self.compare_fn = compare_fn or self._default_compare\n",
    "\n",
    "    def _default_compare(self, str1: str, str2: str) -> bool:\n",
    "        \"\"\"Default string comparison function that ignores case and whitespace.\n",
    "\n",
    "        Args:\n",
    "            str1 (str): First string to compare\n",
    "            str2 (str): Second string to compare\n",
    "\n",
    "        Returns:\n",
    "            bool: True if strings match after normalization\n",
    "        \"\"\"\n",
    "        return str1.strip().lower() == str2.strip().lower()\n",
    "\n",
    "    def evaluate(\n",
    "        self, system_output: str, correct_answer: str, **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"Evaluates whether a system output exactly matches the correct answer.\n",
    "\n",
    "        Args:\n",
    "            system_output (str): The system-generated string.\n",
    "            correct_answer (str): The expected answer string.\n",
    "            **kwargs: Additional keyword arguments (unused).\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: An object with `is_correct` set to True if the normalized strings match,\n",
    "                              along with a corresponding score.\n",
    "        \"\"\"\n",
    "        is_correct = self.compare_fn(system_output, correct_answer)\n",
    "        score = 1.0 if is_correct else 0.0\n",
    "        return EvaluationResult(is_correct=is_correct, score=score)\n",
    "\n",
    "class DiversityScoringEvaluator(IEvaluator[List[str], None]):\n",
    "    \"\"\"\n",
    "    Evaluator to test ensemble outputs -> score them (float)\n",
    "    \"\"\"\n",
    "    def evaluate(\n",
    "            self, \n",
    "            system_output: List[str], \n",
    "            **kwargs) -> EvaluationResult:\n",
    "        if system_output is None or len(system_output) == 0:\n",
    "            return EvaluationResult(is_correct=False, score=-1)\n",
    "\n",
    "        # current compression ratio formula\n",
    "        # TODO: update scoring function to make it better\n",
    "        # -> like use token count\n",
    "\n",
    "        # example I was thinking about:\n",
    "        letter_sum = sum(len(response) for response in system_output)\n",
    "        ratio = compression_ratio(system_output) * min(1, len(system_output)/5) * min(1, letter_sum/100)\n",
    "        # ratio = compression_ratio(system_output, algorithm='gzip',verbose=True)\n",
    "        return EvaluationResult(is_correct=True,score=ratio,metadata = {'responses': system_output})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance (WIP)\n",
    "- need to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    is_correct: bool\n",
    "    score: float\n",
    "    metadata: dict\n",
    "\n",
    "class EditDistanceScoringEvaluator:\n",
    "\n",
    "    def evaluate(self, system_output: List[str], **kwargs) -> EvaluationResult:\n",
    "        if system_output is None or len(system_output) == 0:\n",
    "            return EvaluationResult(is_correct=False, score=-1, metadata={})\n",
    "\n",
    "        diversity_score = self.compute_distance(system_output)\n",
    "\n",
    "        return EvaluationResult(\n",
    "            is_correct=True, \n",
    "            score=diversity_score,\n",
    "            metadata={'responses': system_output}\n",
    "        )\n",
    "\n",
    "    def compute_distance(self, outputs: List[str]) -> float:\n",
    "        n = len(outputs)\n",
    "        if n < 2:\n",
    "            return 0.0\n",
    "\n",
    "        total_distance = 0\n",
    "        pairs = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                dist = Levenshtein.distance(outputs[i], outputs[j])\n",
    "                max_len = max(len(outputs[i]), len(outputs[j]))\n",
    "                normalized_dist = dist / max_len if max_len > 0 else 0 \n",
    "                total_distance += normalized_dist\n",
    "                pairs += 1\n",
    "        \n",
    "        return total_distance / pairs if pairs > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity Score: 0.8301\n",
      "Is Correct: True\n",
      "Metadata: {'responses': ['hi there', 'hi', 'hello', 'yo whatup']}\n"
     ]
    }
   ],
   "source": [
    "distance_evaluator = EditDistanceScoringEvaluator()\n",
    "\n",
    "# input_strs = [\n",
    "#     \";lkjawefopajwiefpoij23jf9aj8sdfj8903jf908j -- Understanding the importance of effective communication in the workplace cannot be overstated. Clear communication fosters a positive environment where people can express their ideas and work together efficiently. When team members understand one another, they can collaborate seamlessly, avoid misunderstandings, and achieve collective goals. Furthermore, communication skills are essential for building trust, resolving conflicts, and ensuring that expectations are clear. Whether through verbal discussions, emails, or presentations, knowing how to convey thoughts in an understandable way is key to success in any professional setting.\",\n",
    "#     \"fej89qw098efjq29f38j0938j20f398jqwe098fjq98wf -- In any workplace, the ability to communicate effectively is crucial for success. When individuals can clearly articulate their ideas and listen actively, it leads to a more productive and harmonious environment. Good communication prevents misunderstandings, aids in team collaboration, and helps in meeting shared objectives. It also plays a vital role in fostering trust among colleagues, resolving disputes, and ensuring transparency. Whether itâ€™s through face-to-face conversations, written messages, or virtual meetings, mastering communication is essential to creating a positive, high-functioning work culture.\",\n",
    "#     \"Effective communication is a cornerstone of a successful work environment. When employees communicate clearly and efficiently, it improves the overall flow of work and enhances collaboration. Clear exchanges of ideas help to eliminate confusion, build mutual trust, and ensure that everyone is aligned in their goals. Additionally, strong communication skills are key to managing conflicts and setting clear expectations among teams. Whether in meetings, emails, or other formats, being able to communicate effectively contributes to a thriving and efficient workplace.\",\n",
    "#     \"The role of communication in the workplace cannot be overlooked. It serves as the foundation for successful teamwork and organizational growth. When team members share information clearly, it promotes a collaborative atmosphere and reduces the risk of errors or misinterpretations. Strong communication is also vital in building relationships, resolving issues, and making sure everyone is on the same page. Whether it's verbal exchanges or written correspondence, honing your ability to communicate well is vital for fostering an effective work environment.\",\n",
    "#     \"Communication within the workplace is a vital element for success. Clear and open communication promotes a cooperative and efficient atmosphere, helping team members to better understand each otherâ€™s ideas and work toward common goals. It reduces confusion, builds trust, and allows for smoother problem-solving when conflicts arise. By conveying thoughts and expectations effectively, individuals can create stronger working relationships and a productive team dynamic. Whether through emails, phone calls, or face-to-face interactions, mastering communication techniques is key for professional achievement.\",\n",
    "# ]\n",
    "\n",
    "input_strs = [\"hi there\", \"hi\", \"hello\", \"yo whatup\"]\n",
    "\n",
    "# input_strs = [\"This is a sample text with lots of repetition.\", \n",
    "#                 \"This is a sample text with lots of repetition.\",\n",
    "#                 \"This is a sample text with lots of repetition.\"]\n",
    "\n",
    "edit_distance = distance_evaluator.evaluate(input_strs)\n",
    "\n",
    "print(f\"Diversity Score: {edit_distance.score:.4f}\")\n",
    "print(f\"Is Correct: {edit_distance.is_correct}\")\n",
    "print(f\"Metadata: {edit_distance.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty Score\n",
    "- need to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    is_correct: bool\n",
    "    score: float\n",
    "    metadata: dict\n",
    "\n",
    "class NoveltyScoringEvaluator:\n",
    "    \n",
    "    def evaluate(self, model: EmbeddingModel, system_output: List[str], **kwargs) -> EvaluationResult:\n",
    "        if not system_output or len(system_output) == 0:\n",
    "            return EvaluationResult(is_correct=False, score=-1, metadata={})\n",
    "\n",
    "        novelty_scores = [self.compute_novelty(r, system_output[:i]) for i, r in enumerate(system_output)]\n",
    "\n",
    "        avg_novelty = sum(novelty_scores) / len(novelty_scores) if novelty_scores else 0.0\n",
    "\n",
    "        return EvaluationResult(\n",
    "            is_correct=True,\n",
    "            score=avg_novelty,\n",
    "            metadata={'responses': system_output, 'novelty_scores': novelty_scores}\n",
    "        )\n",
    "\n",
    "    def compute_novelty(self, response: str, prior_responses: List[str]) -> float:\n",
    "        if not prior_responses:\n",
    "            return 1.0\n",
    "\n",
    "        new_embedding = self.model.embed_text(response)\n",
    "        prior_embeddings = [self.model.embed_text(r) for r in prior_responses]\n",
    "\n",
    "        similarities = [\n",
    "            np.dot(new_embedding, prior_embedding) /\n",
    "            (np.linalg.norm(new_embedding) * np.linalg.norm(prior_embedding))\n",
    "            for prior_embedding in prior_embeddings\n",
    "        ]\n",
    "\n",
    "        return 1 - max(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationResult(is_correct=True, score=0.08368770360509659, metadata={'responses': ['Hello world!', 'Hi there!', 'Goodbye!']})\n"
     ]
    }
   ],
   "source": [
    "novelty_evaluator = NoveltyScoringEvaluator()\n",
    "\n",
    "input_strs = [\"hi there\", \"hi\", \"hello\", \"yo whatup\"]\n",
    "\n",
    "mock_model: MockEmbeddingModel = MockEmbeddingModel()\n",
    "novelty = novelty_evaluator.evaluate(mock_model, input_strs)\n",
    "\n",
    "print(f\"Diversity Score: {novelty.score:.4f}\")\n",
    "print(f\"Is Correct: {novelty.is_correct}\")\n",
    "print(f\"Metadata: {novelty.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mock_model: MockEmbeddingModel = MockEmbeddingModel()\n",
    "cosine: CosineSimilarity = CosineSimilarity()\n",
    "exact_evaluator = ExactMatchEvaluator()\n",
    "diversity_evaluator = DiversityScoringEvaluator()\n",
    "edit_dist_evaluator = EditDistanceScoringEvaluator()\n",
    "\n",
    "def ensemble_diversity(strings):\n",
    "    compression = diversity_evaluator.evaluate(strings)\n",
    "    print(\"DiversityScoringEvaluator result:\", compression)\n",
    "    scores = list()\n",
    "    for ind1 in range(len(strings)):\n",
    "        ind2 = ind1+1 if ind1+1 != len(strings) else 0\n",
    "        curr_score = calculate_text_similarity(text1=strings[ind1], text2=strings[ind2], model=mock_model, metric=cosine)\n",
    "        print(f\"SimilarityScore between ind1={ind1} and ind2={ind2}: {curr_score}\")\n",
    "        scores.append(curr_score)\n",
    "    avg_score = np.average(scores)\n",
    "    print(f\"Avg cosine similarity: {avg_score}\")\n",
    "    print(f\"diversity cosine-sim inverse: {1-avg_score}\")\n",
    "    edit_distance = edit_dist_evaluator.evaluate(strings)\n",
    "    print(f\"edit-dist score: {edit_distance.score:.4f}\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(f\"possible diversity score: {(1-avg_score) * compression.score * edit_distance.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiversityScoringEvaluator result: EvaluationResult(is_correct=True, score=0.063936, metadata={'responses': ['hi there', 'hi', 'hello', 'yo whatup']})\n",
      "SimilarityScore between ind1=0 and ind2=1: 0.5207675658482732\n",
      "SimilarityScore between ind1=1 and ind2=2: 0.6088947130341378\n",
      "SimilarityScore between ind1=2 and ind2=3: 0.67913155770349\n",
      "SimilarityScore between ind1=3 and ind2=0: 0.9344774636399475\n",
      "Avg cosine similarity: 0.6858178250564622\n",
      "diversity cosine-sim inverse: 0.31418217494353784\n",
      "edit-dist score: 0.8301\n",
      "-------------------------------\n",
      "possible diversity score: 0.0166745277343434\n"
     ]
    }
   ],
   "source": [
    "# input_strs = [\n",
    "#     \";lkjawefopajwiefpoij23jf9aj8sdfj8903jf908j -- Understanding the importance of effective communication in the workplace cannot be overstated. Clear communication fosters a positive environment where people can express their ideas and work together efficiently. When team members understand one another, they can collaborate seamlessly, avoid misunderstandings, and achieve collective goals. Furthermore, communication skills are essential for building trust, resolving conflicts, and ensuring that expectations are clear. Whether through verbal discussions, emails, or presentations, knowing how to convey thoughts in an understandable way is key to success in any professional setting.\",\n",
    "#     \"fej89qw098efjq29f38j0938j20f398jqwe098fjq98wf -- In any workplace, the ability to communicate effectively is crucial for success. When individuals can clearly articulate their ideas and listen actively, it leads to a more productive and harmonious environment. Good communication prevents misunderstandings, aids in team collaboration, and helps in meeting shared objectives. It also plays a vital role in fostering trust among colleagues, resolving disputes, and ensuring transparency. Whether itâ€™s through face-to-face conversations, written messages, or virtual meetings, mastering communication is essential to creating a positive, high-functioning work culture.\",\n",
    "#     \"Effective communication is a cornerstone of a successful work environment. When employees communicate clearly and efficiently, it improves the overall flow of work and enhances collaboration. Clear exchanges of ideas help to eliminate confusion, build mutual trust, and ensure that everyone is aligned in their goals. Additionally, strong communication skills are key to managing conflicts and setting clear expectations among teams. Whether in meetings, emails, or other formats, being able to communicate effectively contributes to a thriving and efficient workplace.\",\n",
    "#     \"The role of communication in the workplace cannot be overlooked. It serves as the foundation for successful teamwork and organizational growth. When team members share information clearly, it promotes a collaborative atmosphere and reduces the risk of errors or misinterpretations. Strong communication is also vital in building relationships, resolving issues, and making sure everyone is on the same page. Whether it's verbal exchanges or written correspondence, honing your ability to communicate well is vital for fostering an effective work environment.\",\n",
    "#     \"Communication within the workplace is a vital element for success. Clear and open communication promotes a cooperative and efficient atmosphere, helping team members to better understand each otherâ€™s ideas and work toward common goals. It reduces confusion, builds trust, and allows for smoother problem-solving when conflicts arise. By conveying thoughts and expectations effectively, individuals can create stronger working relationships and a productive team dynamic. Whether through emails, phone calls, or face-to-face interactions, mastering communication techniques is key for professional achievement.\",\n",
    "# ]\n",
    "\n",
    "input_strs = [\"hi there\", \"hi\", \"hello\", \"yo whatup\"]\n",
    "\n",
    "# input_strs = [\"This is a sample text with lots of repetition.\", \n",
    "#                 \"This is a sample text with lots of repetition.\",\n",
    "#                 \"This is a sample text with lots of repetition.\"]\n",
    "\n",
    "ensemble_diversity(input_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke 0: [Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!]\n",
      "Joke 1: [Why don't skeletons fight each other? They don't have the guts!]\n",
      "Joke 2: [Why don't skeletons fight each other?\n",
      "\n",
      "They don't have the guts.]\n",
      "Joke 3: [Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!]\n",
      "Joke 4: [Why don't skeletons fight each other? They don't have the guts.]\n",
      "Joke 5: [Why don't skeletons fight each other? They don't have the guts.]\n",
      "Joke 6: [Why don't skeletons fight each other? They don't have the guts!]\n",
      "Joke 7: [Why donâ€™t skeletons fight each other? They donâ€™t have the guts.]\n",
      "Joke 8: [Why don't skeletons fight each other? They don't have the guts.]\n",
      "Joke 9: [Why don't skeletons fight each other? They don't have the guts.]\n",
      "-----\n",
      "DiversityScoringEvaluator result: EvaluationResult(is_correct=True, score=3.45, metadata={'responses': [\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", \"Why don't skeletons fight each other? They don't have the guts!\", \"Why don't skeletons fight each other?\\n\\nThey don't have the guts.\", \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", \"Why don't skeletons fight each other? They don't have the guts.\", \"Why don't skeletons fight each other? They don't have the guts.\", \"Why don't skeletons fight each other? They don't have the guts!\", 'Why donâ€™t skeletons fight each other? They donâ€™t have the guts.', \"Why don't skeletons fight each other? They don't have the guts.\", \"Why don't skeletons fight each other? They don't have the guts.\"]})\n",
      "SimilarityScore between ind1=0 and ind2=1: 0.8843819811752456\n",
      "SimilarityScore between ind1=1 and ind2=2: 0.9364324737968321\n",
      "SimilarityScore between ind1=2 and ind2=3: 0.8847772710380415\n",
      "SimilarityScore between ind1=3 and ind2=4: 0.8857931367895382\n",
      "SimilarityScore between ind1=4 and ind2=5: 1.0\n",
      "SimilarityScore between ind1=5 and ind2=6: 0.9998557731781514\n",
      "SimilarityScore between ind1=6 and ind2=7: 0.1373464430793195\n",
      "SimilarityScore between ind1=7 and ind2=8: 0.13729294236742365\n",
      "SimilarityScore between ind1=8 and ind2=9: 1.0\n",
      "SimilarityScore between ind1=9 and ind2=0: 0.8857931367895382\n",
      "Avg cosine similarity: 0.775167315821409\n",
      "diversity cosine-sim inverse: 0.22483268417859104\n",
      "edit-dist score: 0.2422\n",
      "-------------------------------\n",
      "possible diversity score: 0.18786989150445282\n"
     ]
    }
   ],
   "source": [
    "num_jokes = 10\n",
    "responses = []\n",
    "\n",
    "for i in range(num_jokes):\n",
    "    res = llm(prompt=\"Tell me a funny joke. Keep it concise.\", model_id=\"openai:gpt-4o\").data\n",
    "    responses.append(res)\n",
    "    print(f\"Joke {i}: [{res}]\")\n",
    "\n",
    "print(\"-----\")\n",
    "ensemble_diversity(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke 0: [Why donâ€™t scientists trust atoms? Because they make up everything! ]\n",
      "Joke 1: [ Parallel lines have so much in common. Itâ€™s a shame theyâ€™ll never meet. ]\n",
      "Joke 2: [ Why did the scarecrow win an award? Because he was outstanding in his field! ]\n",
      "Joke 3: [ I told my wife she was drawing her eyebrows too high. She looked surprised. ]\n",
      "Joke 4: [ Why donâ€™t skeletons fight each other? They donâ€™t have the guts. ]\n",
      "Joke 5: [ What do you call fake spaghetti? An impasta! ]\n",
      "Joke 6: [ Whatâ€™s brown and sticky? A stick! ]\n",
      "Joke 7: [ Why was the math book sad? It had too many problems. ]\n",
      "Joke 8: [ Can February March? No, but April May! ]\n",
      "Joke 9: [ Why was the musician arrested? She got in treble.]\n",
      "-----\n",
      "DiversityScoringEvaluator result: EvaluationResult(is_correct=True, score=1.467, metadata={'responses': ['Why donâ€™t scientists trust atoms? Because they make up everything! ', ' Parallel lines have so much in common. Itâ€™s a shame theyâ€™ll never meet. ', ' Why did the scarecrow win an award? Because he was outstanding in his field! ', ' I told my wife she was drawing her eyebrows too high. She looked surprised. ', ' Why donâ€™t skeletons fight each other? They donâ€™t have the guts. ', ' What do you call fake spaghetti? An impasta! ', ' Whatâ€™s brown and sticky? A stick! ', ' Why was the math book sad? It had too many problems. ', ' Can February March? No, but April May! ', ' Why was the musician arrested? She got in treble.']})\n",
      "SimilarityScore between ind1=0 and ind2=1: 0.03290772299379834\n",
      "SimilarityScore between ind1=1 and ind2=2: 0.2362752728324614\n",
      "SimilarityScore between ind1=2 and ind2=3: 0.8911484218391627\n",
      "SimilarityScore between ind1=3 and ind2=4: 0.23763559138432722\n",
      "SimilarityScore between ind1=4 and ind2=5: 0.08483849065288684\n",
      "SimilarityScore between ind1=5 and ind2=6: 0.10357981441477468\n",
      "SimilarityScore between ind1=6 and ind2=7: 0.21758838510790685\n",
      "SimilarityScore between ind1=7 and ind2=8: 0.740438790870045\n",
      "SimilarityScore between ind1=8 and ind2=9: 0.7932308818518455\n",
      "SimilarityScore between ind1=9 and ind2=0: 0.24304717086492292\n",
      "Avg cosine similarity: 0.35806905428121316\n",
      "diversity cosine-sim inverse: 0.6419309457187868\n",
      "edit-dist score: 0.7335\n",
      "-------------------------------\n",
      "possible diversity score: 0.6907191750125549\n"
     ]
    }
   ],
   "source": [
    "prompts = 1\n",
    "responses = []\n",
    "\n",
    "for i in range(prompts):\n",
    "    res = llm(prompt=\"Tell me 10 jokes. make them split with \\'||\\'. Don't say anything else besides the joke. \", model_id=\"openai:gpt-4o\").data.split('||')\n",
    "    responses += res\n",
    "\n",
    "if prompts == 1 and len(responses) > 1:\n",
    "    for i in range(len(responses)):\n",
    "        print(f\"Joke {i}: [{responses[i]}]\")\n",
    "\n",
    "print(\"-----\")\n",
    "ensemble_diversity(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Improvements TODO\n",
    "- Merge all functions\n",
    "- fix ensembling\n",
    "## Potential other cases to explore\n",
    "- work ensembling all \"diversity\" related metrics \n",
    "  - add more metrics\n",
    "  - tune added metrics\n",
    "- combination of validation/hallucination metric + ensembled diversity metric -> score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
