{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity Testbench\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ember Package Testing (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -e ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, os\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"omitted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing dependencies if current path is <root>/src/ember/examples/diversity_testbench.ipynb\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ember Repo Loads (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig\n",
    "from ember.core.registry.model.config.settings import initialize_ember\n",
    "from ember.core.registry.model.base.services.model_service import ModelService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "model_registry = initialize_ember()\n",
    "print(model_registry.list_models())\n",
    "llm = ModelService(registry=model_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelNotFoundError",
     "evalue": "Model 'openai:gpt-4o' not found. Available models:\n- ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModelNotFoundError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai:gpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/ember-branch2/ember-v2/src/ember/core/registry/model/base/services/model_service.py:100\u001b[39m, in \u001b[36mModelService.invoke_model\u001b[39m\u001b[34m(self, model_id, prompt, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._invoke(model_id, prompt, **kwargs)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/ember-branch2/ember-v2/src/ember/core/registry/model/base/services/model_service.py:104\u001b[39m, in \u001b[36mModelService._invoke\u001b[39m\u001b[34m(self, model_id, prompt, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> ChatResponse:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    106\u001b[39m         response = model(prompt=prompt, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/ember-branch2/ember-v2/src/ember/core/registry/model/base/services/model_service.py:88\u001b[39m, in \u001b[36mModelService.get_model\u001b[39m\u001b[34m(self, model_id)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m     86\u001b[39m     validated_id = raw_id\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m model: Optional[BaseProviderModel] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidated_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidated_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/ember-branch2/ember-v2/src/ember/core/registry/model/base/registry/model_registry.py:142\u001b[39m, in \u001b[36mModelRegistry.get_model\u001b[39m\u001b[34m(self, model_id)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_infos:\n\u001b[32m    141\u001b[39m     available_models: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._model_infos.keys())\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelNotFoundError(\n\u001b[32m    143\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found. Available models:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_models\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    144\u001b[39m     )\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._models:\n\u001b[32m    146\u001b[39m     model_info = \u001b[38;5;28mself\u001b[39m._model_infos[model_id]\n",
      "\u001b[31mModelNotFoundError\u001b[39m: Model 'openai:gpt-4o' not found. Available models:\n- "
     ]
    }
   ],
   "source": [
    "response = llm(prompt=\"Hello!\", model_id=\"openai:gpt-4o\")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Neural Similarity Scoring - Cosine Similarity\n",
    "\n",
    "from `src/ember/core/utils/embedding_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'Hello world!' and 'Hello, world??': 0.9150491464734943\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Protocol\n",
    "import math\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 1) Embedding Model Interfaces & Implementations\n",
    "################################################################\n",
    "\n",
    "\n",
    "class EmbeddingModel(Protocol):\n",
    "    \"\"\"Interface for embedding models.\n",
    "\n",
    "    This protocol defines the minimal interface required to compute a text\n",
    "    embedding. Implementations may use local models, external APIs, or custom\n",
    "    neural networks.\n",
    "\n",
    "    Methods:\n",
    "        embed_text: Compute the embedding for a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Computes the embedding vector for the provided text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list of floats representing the embedding vector.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "class Text_Embedding_3_EmbeddingModel(Protocol):\n",
    "    \"\"\"Interface for embedding models.\n",
    "\n",
    "    This protocol defines the minimal interface required to compute a text\n",
    "    embedding. Implementations may use local models, external APIs, or custom\n",
    "    neural networks.\n",
    "\n",
    "    Methods:\n",
    "        embed_text: Compute the embedding for a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initializes the embedding model with the OpenAI API key.\n",
    "\n",
    "        Args:\n",
    "            api_key (str): OpenAI API key for authentication.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenAI API key must be provided or set in the environment variable OPENAI_API_KEY.\")\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Computes the embedding vector for the provided text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list of floats representing the embedding vector.\n",
    "        \"\"\"\n",
    "        response = openai.Embedding.create(\n",
    "            model=\"text-embedding-3\",\n",
    "            input=text\n",
    "        )\n",
    "        return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "class MockEmbeddingModel:\n",
    "    \"\"\"Mock implementation of an embedding model using naive ASCII encoding.\n",
    "\n",
    "    This simple model converts each character in the text to a normalized ASCII\n",
    "    value. It is intended solely for demonstration and testing purposes.\n",
    "\n",
    "    Methods:\n",
    "        embed_text: Converts text to a sequence of normalized ASCII values.\n",
    "    \"\"\"\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Embeds text by converting each character to its normalized ASCII code.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list of floats representing the embedding. Returns an\n",
    "            empty list if the text is empty.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        return [ord(ch) / 256.0 for ch in text]\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 2) Similarity Metric Interface & Implementations\n",
    "################################################################\n",
    "\n",
    "\n",
    "class SimilarityMetric(ABC):\n",
    "    \"\"\"Abstract base class for computing similarity between embedding vectors.\n",
    "\n",
    "    Subclasses must implement the similarity method to calculate a similarity\n",
    "    score between two vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
    "        \"\"\"Calculates the similarity between two embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            vec_a (List[float]): The first embedding vector.\n",
    "            vec_b (List[float]): The second embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The similarity score, typically in the range [0, 1] or [-1, 1].\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class CosineSimilarity(SimilarityMetric):\n",
    "    \"\"\"Implementation of cosine similarity for embedding vectors.\n",
    "\n",
    "    The cosine similarity is defined as:\n",
    "        similarity(a, b) = (a Â· b) / (||a|| * ||b||)\n",
    "\n",
    "    Returns 0.0 if either vector is empty or if any vector's norm is zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def similarity(self, vec_a: List[float], vec_b: List[float]) -> float:\n",
    "        \"\"\"Computes cosine similarity between two embedding vectors.\n",
    "\n",
    "        Args:\n",
    "            vec_a (List[float]): The first embedding vector.\n",
    "            vec_b (List[float]): The second embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The cosine similarity score.\n",
    "        \"\"\"\n",
    "        if not vec_a or not vec_b:\n",
    "            return 0.0\n",
    "\n",
    "        dot_product: float = sum(a * b for a, b in zip(vec_a, vec_b))\n",
    "        norm_a: float = math.sqrt(sum(a * a for a in vec_a))\n",
    "        norm_b: float = math.sqrt(sum(b * b for b in vec_b))\n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 3) High-Level Utility Function\n",
    "################################################################\n",
    "\n",
    "\n",
    "def calculate_text_similarity(\n",
    "    text1: str, text2: str, model: EmbeddingModel, metric: SimilarityMetric\n",
    ") -> float:\n",
    "    \"\"\"Calculates text similarity using an embedding model and a similarity metric.\n",
    "\n",
    "    This function generates embeddings for the provided texts and then computes a\n",
    "    similarity score using the given similarity metric.\n",
    "\n",
    "    Args:\n",
    "        text1 (str): The first text string.\n",
    "        text2 (str): The second text string.\n",
    "        model (EmbeddingModel): An instance conforming to the embedding model interface.\n",
    "        metric (SimilarityMetric): An instance implementing a similarity metric.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed similarity score.\n",
    "    \"\"\"\n",
    "    embedding1: List[float] = model.embed_text(text=text1)\n",
    "    embedding2: List[float] = model.embed_text(text=text2)\n",
    "    return metric.similarity(vec_a=embedding1, vec_b=embedding2)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# 4) Example Usage (Executable as Script)\n",
    "################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    mock_model: MockEmbeddingModel = MockEmbeddingModel()\n",
    "    cosine: CosineSimilarity = CosineSimilarity()\n",
    "\n",
    "    text_a: str = \"Hello world!\"\n",
    "    text_b: str = \"Hello, world??\"\n",
    "\n",
    "    score: float = calculate_text_similarity(\n",
    "        text1=text_a, text2=text_b, model=mock_model, metric=cosine\n",
    "    )\n",
    "    print(f\"Similarity between '{text_a}' and '{text_b}': {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Compression Ratio (WIP)\n",
    "\n",
    "from `src/ember/core/utils/eval/evaluators.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, TypeVar, Optional, List, Generic, Callable, Union\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IEvaluator, EvaluationResult\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextractors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RegexExtractor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiversity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compression_ratio\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "from typing import Any, Dict, TypeVar, Optional, List, Generic, Callable, Union\n",
    "\n",
    "from .base_evaluator import IEvaluator, EvaluationResult\n",
    "from .extractors import RegexExtractor\n",
    "\n",
    "from diversity import compression_ratio\n",
    "\n",
    "T_out = TypeVar(\"T_out\")\n",
    "T_truth = TypeVar(\"T_truth\")\n",
    "\n",
    "\n",
    "class ComposedEvaluator(IEvaluator[T_out, T_truth], Generic[T_out, T_truth]):\n",
    "    \"\"\"Combines an output extractor with an evaluator for the extracted data.\n",
    "\n",
    "    This evaluator first transforms the system output using the provided extractor,\n",
    "    then evaluates the extracted value using the specified base evaluator.\n",
    "\n",
    "    Args:\n",
    "        extractor: An object with an `extract` method to process the system output.\n",
    "        base_evaluator (IEvaluator): An evaluator that processes the extracted output.\n",
    "\n",
    "    Returns:\n",
    "        EvaluationResult: The result of the evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        extractor: Any,  # Expecting an extractor with an `extract` method.\n",
    "        base_evaluator: IEvaluator[Any, Any],\n",
    "    ) -> None:\n",
    "        self.extractor = extractor\n",
    "        self.base_evaluator = base_evaluator\n",
    "\n",
    "    def evaluate(\n",
    "        self, system_output: T_out, correct_answer: Any, **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"Evaluates the provided system output against the correct answer.\n",
    "\n",
    "        Args:\n",
    "            system_output (T_out): The raw output generated by the system.\n",
    "            correct_answer (Any): The expected correct answer.\n",
    "            **kwargs: Additional keyword arguments for extraction or evaluation.\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: The result of evaluating the extracted value.\n",
    "        \"\"\"\n",
    "        extracted_value = self.extractor.extract(system_output, **kwargs)\n",
    "        return self.base_evaluator.evaluate(extracted_value, correct_answer, **kwargs)\n",
    "\n",
    "\n",
    "# Basic Evaluators\n",
    "\n",
    "\n",
    "class ExactMatchEvaluator(IEvaluator[str, str]):\n",
    "    \"\"\"Evaluator to check for an exact match between two strings,\n",
    "    ignoring differences in whitespace and case.\n",
    "\n",
    "    Example:\n",
    "        evaluator = ExactMatchEvaluator()\n",
    "        result = evaluator.evaluate(\"Hello World\", \"hello   world\")\n",
    "\n",
    "    Args:\n",
    "        compare_fn (Optional[Callable[[str, str], bool]]): Optional custom comparison function.\n",
    "            If not provided, strings are normalized (whitespace removed, lowercase) before comparison.\n",
    "\n",
    "    Returns:\n",
    "        EvaluationResult: The result containing a correctness flag and a score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, compare_fn: Optional[Callable[[str, str], bool]] = None) -> None:\n",
    "        self.compare_fn = compare_fn or self._default_compare\n",
    "\n",
    "    def _default_compare(self, str1: str, str2: str) -> bool:\n",
    "        \"\"\"Default string comparison function that ignores case and whitespace.\n",
    "\n",
    "        Args:\n",
    "            str1 (str): First string to compare\n",
    "            str2 (str): Second string to compare\n",
    "\n",
    "        Returns:\n",
    "            bool: True if strings match after normalization\n",
    "        \"\"\"\n",
    "        return str1.strip().lower() == str2.strip().lower()\n",
    "\n",
    "    def evaluate(\n",
    "        self, system_output: str, correct_answer: str, **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"Evaluates whether a system output exactly matches the correct answer.\n",
    "\n",
    "        Args:\n",
    "            system_output (str): The system-generated string.\n",
    "            correct_answer (str): The expected answer string.\n",
    "            **kwargs: Additional keyword arguments (unused).\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: An object with `is_correct` set to True if the normalized strings match,\n",
    "                              along with a corresponding score.\n",
    "        \"\"\"\n",
    "        is_correct = self.compare_fn(system_output, correct_answer)\n",
    "        score = 1.0 if is_correct else 0.0\n",
    "        return EvaluationResult(is_correct=is_correct, score=score)\n",
    "\n",
    "\n",
    "class NumericToleranceEvaluator(IEvaluator[float, float]):\n",
    "    \"\"\"Evaluator to check if a numeric output is within a specified tolerance of the expected value.\n",
    "\n",
    "    Example:\n",
    "        evaluator = NumericToleranceEvaluator(tolerance=0.05)\n",
    "        result = evaluator.evaluate(3.14159, 3.14)\n",
    "\n",
    "    Args:\n",
    "        tolerance (float): The maximum allowed difference between the output and the correct value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tolerance: float = 0.01) -> None:\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def evaluate(\n",
    "        self, system_output: float, correct_answer: float, **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"Evaluates the numeric system output against the correct value within a specified tolerance.\n",
    "\n",
    "        Args:\n",
    "            system_output (float): The numeric output from the system.\n",
    "            correct_answer (float): The expected numeric answer.\n",
    "            **kwargs: Additional keyword arguments (unused).\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: The result including a correctness flag, score, and metadata about the difference.\n",
    "        \"\"\"\n",
    "        difference = abs(system_output - correct_answer)\n",
    "        # Round to handle floating point precision issues\n",
    "        rounded_diff = round(difference, 8)\n",
    "        is_correct = rounded_diff <= self.tolerance\n",
    "        base = abs(correct_answer) if correct_answer != 0 else 1.0\n",
    "        score = max(0.0, 1.0 - rounded_diff / base)\n",
    "        return EvaluationResult(\n",
    "            is_correct=is_correct, score=score, metadata={\"diff\": rounded_diff}\n",
    "        )\n",
    "\n",
    "\n",
    "class CodeExecutionEvaluator(IEvaluator[str, str]):\n",
    "    \"\"\"Evaluator that executes Python code and compares its standard output to an expected result.\n",
    "\n",
    "    **WARNING**: Executing arbitrary code is dangerous.\n",
    "    Only use this evaluator with fully trusted code strings.\n",
    "\n",
    "    Args:\n",
    "        timeout (float): Maximum duration (in seconds) to allow code execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timeout: float = 5.0) -> None:\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def evaluate(\n",
    "        self, system_output: str, correct_answer: str, **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"Executes the provided Python code and compares its standard output to the expected result.\n",
    "\n",
    "        Args:\n",
    "            system_output (str): A Python code string to be executed.\n",
    "            correct_answer (str): The expected output from the code execution.\n",
    "            **kwargs: Additional keyword arguments (unused).\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: The result of execution, including stdout, stderr, and exit code in metadata.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            process_result: subprocess.CompletedProcess = subprocess.run(\n",
    "                args=[\"python\", \"-c\", system_output],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.timeout,\n",
    "            )\n",
    "            stdout_str = process_result.stdout.strip()\n",
    "            expected_str = correct_answer.strip()\n",
    "            is_correct = stdout_str == expected_str\n",
    "            return EvaluationResult(\n",
    "                is_correct=is_correct,\n",
    "                score=1.0 if is_correct else 0.0,\n",
    "                metadata={\n",
    "                    \"stdout\": process_result.stdout,\n",
    "                    \"stderr\": process_result.stderr,\n",
    "                    \"exit_code\": process_result.returncode,\n",
    "                },\n",
    "            )\n",
    "        except subprocess.TimeoutExpired as timeout_error:\n",
    "            return EvaluationResult(\n",
    "                is_correct=False,\n",
    "                score=0.0,\n",
    "                metadata={\"error\": f\"TimeoutExpired: {str(timeout_error)}\"},\n",
    "            )\n",
    "        except Exception as error:\n",
    "            return EvaluationResult(\n",
    "                is_correct=False,\n",
    "                score=0.0,\n",
    "                metadata={\"error\": f\"{type(error).__name__}: {str(error)}\"},\n",
    "            )\n",
    "\n",
    "class DiversityScoringEvaluator(IEvaluator[List[str], None]):\n",
    "    \"\"\"\n",
    "    Evaluator to test ensemble outputs -> score them (float)\n",
    "    \"\"\"\n",
    "    def evaluate(\n",
    "            self, \n",
    "            system_output: List[str], \n",
    "            **kwargs) -> EvaluationResult:\n",
    "        if system_output is None or len(system_output) == 0:\n",
    "            return EvaluationResult(is_correct=False, score=-1)\n",
    "\n",
    "        # current compression ratio formula\n",
    "        # TODO: update scoring function to make it better\n",
    "        # -> like use token count\n",
    "\n",
    "        # example I was thinking about:\n",
    "        # letter_sum = sum(len(response) for response in system_output)\n",
    "        # ratio = compression_ratio(system_output) * min(1, len(system_output)/5) * min(1, letter_sum/100)\n",
    "        ratio = compression_ratio(system_output, algorithm='gzip',verbose=True)\n",
    "        return EvaluationResult(is_correct=True,score=ratio,metadata = {'responses': system_output})\n",
    "\n",
    "# Composite Evaluator Example\n",
    "\n",
    "\n",
    "class PartialRegexEvaluator(ComposedEvaluator[str, str]):\n",
    "    \"\"\"Evaluator that uses a regex extractor followed by an exact match evaluation.\n",
    "\n",
    "    First, it extracts a substring using a regular expression, then checks if the extracted\n",
    "    value matches the expected answer exactly.\n",
    "\n",
    "    Args:\n",
    "        pattern (str): The regular expression pattern used for extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pattern: str) -> None:\n",
    "        extractor = RegexExtractor(pattern)\n",
    "        evaluator = ExactMatchEvaluator()\n",
    "        super().__init__(extractor=extractor, base_evaluator=evaluator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Direct final-output comparison (exact match)\n",
    "    exact_evaluator = ExactMatchEvaluator()\n",
    "    result_exact = exact_evaluator.evaluate(\"Hello World\", \"hello  world\")\n",
    "    print(\"ExactMatch result:\", result_exact)\n",
    "\n",
    "    # Example 2: Numeric tolerance evaluation\n",
    "    numeric_evaluator = NumericToleranceEvaluator(tolerance=0.05)\n",
    "    result_numeric = numeric_evaluator.evaluate(3.14159, 3.14)\n",
    "    print(\"NumericTolerance result:\", result_numeric)\n",
    "\n",
    "    # Example 3: Composite evaluator with regex extraction and exact matching.\n",
    "    regex_pattern = r\"answer\\s+is\\s+(\\w+)\"\n",
    "    partial_regex_evaluator = PartialRegexEvaluator(pattern=regex_pattern)\n",
    "    result_regex = partial_regex_evaluator.evaluate(\"The answer is PARIS\", \"PARIS\")\n",
    "    print(\"PartialRegexEvaluator result:\", result_regex)\n",
    "\n",
    "    # Example 4: Code execution evaluator.\n",
    "    code_evaluator = CodeExecutionEvaluator()\n",
    "    code_string = \"print('Hello')\"\n",
    "    result_code = code_evaluator.evaluate(code_string, \"Hello\")\n",
    "    print(\"CodeExecutionEvaluator result:\", result_code)\n",
    "\n",
    "    #TODO Example 5: Diversity Scoring evaluator.\n",
    "    diversity_evaluator = DiversityScoringEvaluator()\n",
    "    # input_strs = [\"hi there\", \"hi\", \"hello\", \"yo whatup\"]\n",
    "    input_strs = [\"This is a sample text with lots of repetition.\", \n",
    "                  \"This is a sample text with lots of repetition.\",\n",
    "                  \"This is a sample text with lots of repetition.\"]\n",
    "    result_diversity = diversity_evaluator.evaluate(input_strs)\n",
    "    print(\"DiversityScoringEvaluator result:\", result_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Potential other cases to explore\n",
    "-  Edit distance\n",
    "- ensembling all \"diversity\" related metrics\n",
    "- combination of validation/hallucination metric + ensembled diversity metric -> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ember-ai-lWB6Kps4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
